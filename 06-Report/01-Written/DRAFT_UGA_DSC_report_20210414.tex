% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Credit Lending Decisions},
  pdfauthor={Cody Dailey; Ishaan Dave1; Yang Ge1; Vipul Shinde},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Credit Lending Decisions}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{2021 University of Georgia Data Science Competition}
\author{Cody Dailey\footnote{Department of Epidemiology and Biostatistics} \and Ishaan Dave\textsuperscript{1} \and Yang Ge\textsuperscript{1} \and Vipul Shinde\footnote{Department of Computer Science}}
\date{format(Sys.Date(), ``\%d \%B \%Y'')}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\begin{verbatim}
## 
## Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     filter, lag
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union
\end{verbatim}

\begin{verbatim}
## [[1]]
## [1] "dplyr"     "stats"     "graphics"  "grDevices" "utils"     "datasets" 
## [7] "methods"   "base"     
## 
## [[2]]
## [1] "flextable" "dplyr"     "stats"     "graphics"  "grDevices" "utils"    
## [7] "datasets"  "methods"   "base"     
## 
## [[3]]
##  [1] "knitr"     "flextable" "dplyr"     "stats"     "graphics"  "grDevices"
##  [7] "utils"     "datasets"  "methods"   "base"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dir}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "DRAFT_UGA_DSC_report_20210414.log" "DRAFT_UGA_DSC_report_20210414.tex"
## [3] "methods DSC .docx"                 "README.md"                        
## [5] "UGA_DSC_report.Rmd"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "C:/Users/daile/Documents/GitHub Projects/UGA-Data-Science-Competition/06-Report/01-Written"
\end{verbatim}

For more details, please see appendix (add exactly where -- figure XXX).

\newpage

\hypertarget{preface}{%
\section*{Preface}\label{preface}}
\addcontentsline{toc}{section}{Preface}

\newpage

\hypertarget{part-background}{%
\part{Background}\label{part-background}}

\hypertarget{competition-prompt}{%
\section{Competition Prompt}\label{competition-prompt}}

\hypertarget{overview-of-sectioning}{%
\section{Overview of Sectioning}\label{overview-of-sectioning}}

\newpage

\hypertarget{part-bank-xyz-and-credit-bureaus-credit-data}{%
\part{Bank XYZ and Credit Bureaus' Credit Data}\label{part-bank-xyz-and-credit-bureaus-credit-data}}

\hypertarget{univariate-descriptions}{%
\section{Univariate Descriptions}\label{univariate-descriptions}}

Table 1

\hypertarget{exploratory-data-analysis}{%
\section{Exploratory Data Analysis}\label{exploratory-data-analysis}}

\hypertarget{methods}{%
\subsection{Methods}\label{methods}}

Data were summarized using means and standard deviations or frequency and percentages as appropriate. For discrete count data, data were summarized using medians and interquartile range. Additional exploratory data analyses were conducted to examine normality of continuous variables and cross-tabulations of categorical variables between outcome levels. Bivariate relationships were examined using correlations (point-biserial between categorical and continuous, and Pearson correlation between continuous variables) and plotted in a correlation matrix (darker colors represent stronger correlations). To predict whether or not individuals will default, 2 models were fit: the first was an adaptive LASSO logistic regression model, and the second was a random forest machine learning algorithm for classification. Their predictive performances were compared by examining differences in area under the curve (AUC) and receiver operating curves (ROC), and the financial impact of correct (or incorrect) classification was assessed by assigning a cost to correctly or incorrectly classifying an individual.

\hypertarget{results}{%
\subsection{Results}\label{results}}

Figure Correlation Matrix

\newpage

\hypertarget{part-modeling-and-predicting-probabilities-of-account-default}{%
\section{(PART) Modeling and Predicting Probabilities of Account Default}\label{part-modeling-and-predicting-probabilities-of-account-default}}

\hypertarget{logistic-regression}{%
\section{Logistic Regression}\label{logistic-regression}}

Our main outcome of interest is whether or not an individual defaults when asking for a loan. Traditionally, associations between predictors and outcome are tested individually through univariate regression models, then a subset of those covariates are selected for a multivariate regression analysis if univariate analyses yielded significant results. Given the large number of possible predictors, we suspected severe multicollinarity between some pairs of variables, and thus, overfitting of our model. To alleviate this, we ``penalized'' regression coefficients to select a parsimonious set of variables that leads to similar prediction results.

\hypertarget{methods-1}{%
\subsection{Methods}\label{methods-1}}

In a normal logistic regression with a binary outcome, our model is defined by logit(E(Y│X)=β\^{}T (X), where Y is the binary outcome (whether or not an individual defaulted), X = (1,X\_1,\ldots,X\_p) is the vector of covariate values, and β = (β\_0,β\_1,\ldots,β\_p ) is the vector of regression parameters. In a penalized regression, estimation is generally in the form of an optimization problem that seeks to maximize the function in the form of L(β)-λ*pen(β), where pen(β) is the ``penalty'' and λ is the ``tuning parameter''. The penalty refers to a constraint surrounding the betas and the tuning parameter corresponds to the amount of ``shrinkage'' applied to the coefficient. In this analysis, an adaptive LASSO (least absolute shrinkage and selection operator) with logistic regression was employed to estimate coefficients and select the most important predictors. Adaptive LASSO is conducted in 2 parts -- the first is a ``ridge'' regression is conducted that uses a penalty proportional to the sum of squares of the regression coefficients and efficiently handles multicollinearity (Equation 1).

(Equation 1)

The second step is the LASSO step that imposes a constraint on the sum of the absolute value of the regression coefficients and uses data-dependent weights (the inverse of the ridge regression coefficients) to penalize regression coefficients accordingly (Equation 2). Therefore strong predictors' coefficients are shrunk less than coefficients of weak predictors.

(Equation 2).

Continuous predictors were standardized to have mean 0 and standard deviation 1 before penalized regression was conducted. The main focus of penalized regression is predictive performance rather than uncertainty around the coefficient itself. Confidence intervals for coefficients and predicted probabilities from penalized methods are problematic because coefficients are already biased toward the null because of the inherent penalty term and not presented with regression coefficients. In the table below, we show the standardized coefficient estimates for both ordinary logistic regression and adaptive lasso as well as the shrinkage associated with each coefficient.

The main purpose of LASSO is predictive performance rather than coefficient estimates. We used a training dataset to best inform the penalty and tuning parameters to be used in the LASSO. A validation test set was used to best determine the parameters in our model and our final model's predictive performance was determined by running our model on a testing dataset and record the resulting AUC associated with classifying an individual as one who will or will not default. Moreover, a confusion matrix of actual vs.~predicted counts of those who did and did not default was constructed to calculate predictive accuracy, classifications were performed at multiple predicted probability thresholds to determine that which resulted in highest predictive accuracy. To quantify predictive performance of our models, a monetary value was associated with both correctly and incorrectly classifying an individual.
\[ CODY ADD SOMETHING HERE ABOUT MONEY / COST FUNCTION \]

In addition to an adaptive LASSO logistic regression model, a random forest machine learning algorithm was run to compare predictive accuracy

\hypertarget{results-1}{%
\subsection{Results}\label{results-1}}

Figure Adaptive LASSO
- Training --\textgreater{} ROC \& Cost on training and validation datasets

Table
- Coefficients / ORs / Probabilities
- Confusion Matrices

\hypertarget{random-forest}{%
\section{Random Forest}\label{random-forest}}

\hypertarget{methods-2}{%
\subsection{Methods}\label{methods-2}}

\hypertarget{results-2}{%
\subsection{Results}\label{results-2}}

Figure
- Training --\textgreater{} ROC \& Cost on training and validation datasets
- Variable importance
- Proximity?
- ``Best'' tree?

Table
- Confusion matrices

\hypertarget{model-comparisons}{%
\section{Model Comparisons}\label{model-comparisons}}

\hypertarget{accuracy}{%
\subsection{Accuracy}\label{accuracy}}

Figure
- ROC \& Cost on test data

\hypertarget{other-considerations}{%
\subsection{Other Considerations}\label{other-considerations}}

Transparency
\ldots{}

\hypertarget{selection}{%
\subsection{Selection}\label{selection}}

\newpage

\hypertarget{part-discussion}{%
\section{(PART) Discussion}\label{part-discussion}}

\hypertarget{scenario-application}{%
\section{Scenario Application}\label{scenario-application}}

\textbf{\emph{Do customers who already have an account with Bank XYZ receive any favorable treatment?}}

Was it picked up in any of our models?
What advantage might it have that wouldn't be evident in data?

\textbf{\emph{Suppose a credit card application is rejected, how would you explain the results to the customer?}}

For TP and FP, does it differ?

Figure
- ``Tipping point'' plots (if we took any value from specific applicant and replaced with mean of someone who would have been approved, does it change probability enough to cross threshold? iterative combinations of strongest more variable predictors or z-score of particular customer?)

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

Dilemma of basing credit approvals on default data (inherently missing approvals that should have been made, identifying only those that shouldn't have been)

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\newpage

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{extended-exploratory-data-analysis}{%
\subsection{Extended Exploratory Data Analysis}\label{extended-exploratory-data-analysis}}

\hypertarget{training-details}{%
\subsection{Training Details}\label{training-details}}

\hypertarget{class-imbalance}{%
\subsubsection{Class Imbalance}\label{class-imbalance}}

\hypertarget{cutoff-thresholds}{%
\subsubsection{Cutoff Thresholds}\label{cutoff-thresholds}}

\hypertarget{cost-rationale}{%
\subsection{Cost Rationale}\label{cost-rationale}}

\hypertarget{table-1}{%
\section{Table 1}\label{table-1}}

\end{document}
