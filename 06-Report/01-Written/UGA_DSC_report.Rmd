---
title: "Credit Lending Decisions"
subtitle: 2021 University of Georgia Data Science Competition
author: 
  - Cody Dailey[^epidbios]
  - Ishaan Dave^1^
  - Yang Ge^1^
  - Vipul Shinde[^compsci]
  
date: '`r format(Sys.Date(), "%d %B %Y")`'

bibliography: ["../../00-References/references.bib"]
biblio-style: apalike
csl: "../../00-References/epidemiology.csl"
link-citations: yes

output: 
  bookdown::word_document2:
    
    toc: true
    toc_depth: 2
    number_sections: true
editor_options: 
  chunk_output_type: console
  
knit: (
  function(inputFile, encoding) { 
    rmarkdown::render( 
      input       = inputFile, 
      encoding    = encoding,       
      output_file = paste0("DRAFT_UGA_DSC_report_", format(Sys.Date(), "%Y%m%d"))
      ) 
      }
      )
---

[^epidbios]:Department of Epidemiology and Biostatistics
[^compsci]:Department of Computer Science


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
my.packages <- c("dplyr", "flextable", "knitr")
lapply(my.packages, library, character.only=T)
```





```{r echo = T}

dir()

getwd()

```




For more details, please see appendix (add exactly where – figure XXX).










\newpage
# Preface {-}


\newpage
# (PART) Background {-}

# Competition Prompt

# Overview of Sectioning








\newpage
# (PART) Bank XYZ and Credit Bureaus' Credit Data {-}

# Univariate Descriptions

Table \@ref(tab:table1)

```{r table1}
load("../../04-Data-Visualization/01-Tables/table_1s.rds")

# training data
## need to remake tables and round their numeric values
flextable(table.1s[[2]]) %>% set_caption("Descriptive Statistics of Training Data")

```



# Exploratory Data Analysis

## Methods

Data were summarized using means and standard deviations or frequency and percentages as appropriate. For discrete count data, data were summarized using medians and interquartile range. Additional exploratory data analyses were conducted to examine normality of continuous variables and cross-tabulations of categorical variables between outcome levels. Bivariate relationships were examined using correlations (point-biserial between categorical and continuous, and Pearson correlation between continuous variables) and plotted in a correlation matrix (darker colors represent stronger correlations). To predict whether or not individuals will default, 2 models were fit: the first was an adaptive LASSO logistic regression model, and the second was a random forest machine learning algorithm for classification. Their predictive performances were compared by examining differences in area under the curve (AUC) and receiver operating curves (ROC), and the financial impact of correct (or incorrect) classification was assessed by assigning a cost to correctly or incorrectly classifying an individual. 



## Results

Figure Correlation Matrix





\newpage
# (PART) Modeling and Predicting Probabilities of Account Default

# Logistic Regression

Our main outcome of interest is whether or not an individual defaults when asking for a loan. Traditionally, associations between predictors and outcome are tested individually through univariate regression models, then a subset of those covariates are selected for a multivariate regression analysis if univariate analyses yielded significant results. Given the large number of possible predictors, we suspected severe multicollinarity between some pairs of variables, and thus, overfitting of our model. To alleviate this, we “penalized” regression coefficients to select a parsimonious set of variables that leads to similar prediction results. 


## Methods


In a normal logistic regression with a binary outcome, our model is defined by logit(E(Y│X)=β^T (X), where Y is the binary outcome (whether or not an individual defaulted), X = (1,X_1,…,X_p) is the vector of covariate values, and β = (β_0,β_1,…,β_p ) is the vector of regression parameters. In a penalized regression, estimation is generally in the form of an optimization problem that seeks to maximize the function in the form of L(β)-λ*pen(β), where pen(β) is the “penalty” and λ is the “tuning parameter”. The penalty refers to a constraint surrounding the betas and the tuning parameter corresponds to the amount of “shrinkage” applied to the coefficient. In this analysis, an adaptive LASSO (least absolute shrinkage and selection operator) with logistic regression was employed to estimate coefficients and select the most important predictors. Adaptive LASSO is conducted in 2 parts – the first is a “ridge” regression is conducted that uses a penalty proportional to the sum of squares of the regression coefficients and efficiently handles multicollinearity (Equation 1).

  (Equation 1)

The second step is the LASSO step that imposes a constraint on the sum of the absolute value of the regression coefficients and uses data-dependent weights (the inverse of the ridge regression coefficients) to penalize regression coefficients accordingly (Equation 2). Therefore strong predictors’ coefficients are shrunk less than coefficients of weak predictors. 

  (Equation 2). 

Continuous predictors were standardized to have mean 0 and standard deviation 1 before penalized regression was conducted. The main focus of penalized regression is predictive performance rather than uncertainty around the coefficient itself. Confidence intervals for coefficients and predicted probabilities from penalized methods are problematic because coefficients are already biased toward the null because of the inherent penalty term and not presented with regression coefficients. In the table below, we show the standardized coefficient estimates for both ordinary logistic regression and adaptive lasso as well as the shrinkage associated with each coefficient.

The main purpose of LASSO is predictive performance rather than coefficient estimates. We used a training dataset to best inform the penalty and tuning parameters to be used in the LASSO. A validation test set was used to best determine the parameters in our model and our final model's predictive performance was determined by running our model on a testing dataset and record the resulting AUC associated with classifying an individual as one who will or will not default. Moreover, a confusion matrix of actual vs. predicted counts of those who did and did not default was constructed to calculate predictive accuracy, classifications were performed at multiple predicted probability thresholds to determine that which resulted in highest predictive accuracy. To quantify predictive performance of our models, a monetary value was associated with both correctly and incorrectly classifying an individual.
$$ CODY ADD SOMETHING HERE ABOUT MONEY / COST FUNCTION $$

In addition to an adaptive LASSO logistic regression model, a random forest machine learning algorithm was run to compare predictive accuracy 

## Results


Figure Adaptive LASSO
- Training --> ROC & Cost on training and validation datasets

Table
- Coefficients / ORs / Probabilities
- Confusion Matrices



Figure \@ref(fig:lasso-auc)



```{r lasso-auc, fig.width=7.5, fig.height=9/16*7.5, fig.cap = "Adaptive LASSO Logistic Regression Hyperparameters and Areas Under the Receiver Operating Characteristic Curve"}

include_graphics("../../04-Data-Visualization/02-Figures/alasso_models_auc.png")


```


Figure \@ref(fig:lasso-profit)


```{r lasso-profit, fig.width=7.5, fig.height=9/16*7.5, fig.cap = "Adaptive LASSO Logistic Regression Hyperparameters and Estimated Monthly Profit per Applicant"}

include_graphics("../../04-Data-Visualization/02-Figures/alasso_models_profit.png")


```






# Random Forest

## Methods


## Results






Figure
- Training --> ROC & Cost on training and validation datasets
- Variable importance
- Proximity?
- "Best" tree?

Table
- Confusion matrices

Figure \@ref(fig:rf-auc)

```{r rf-auc, fig.width=7.5, fig.height=9/16*7.5, fig.cap = "Random Forest Hyperparameters and Areas Under the Receiver Operating Characteristic Curve"}

include_graphics("../../04-Data-Visualization/02-Figures/rf_models_auc.png")


```


Figure \@ref(fig:rf-profit)

```{r rf-profit, fig.width=7.5, fig.height=9/16*7.5, fig.cap = "Random Forest Hyperparameters and Estimated Monthly Profit per Applicant"}

include_graphics("../../04-Data-Visualization/02-Figures/rf_models_profit.png")


```








Figure \@ref(fig:rf-importance)

```{r rf-importance, fig.width=7.5, fig.height=9/16*7.5, fig.cap = "Variable Importance in Random Forest Model"}

include_graphics("../../04-Data-Visualization/02-Figures/rf_variable_importance.png")


```










# Model Comparisons



```{r}
load("../../04-Data-Visualization/01-Tables/comparisons.rdata")

```


Table \@ref(tab:lasso-confusion)

```{r lasso-confusion}

l.cm.ft %>% set_caption("Confusion Matrix for Adaptive LASSO Logistic Regression Model Predictions on the Test Data")

```


Table \@ref(tab:rf-confusion)

```{r rf-confusion}

r.cm.ft %>% set_caption("Confusion Matrix for Random Forest Model Predictions on the Test Data")

```




Table \@ref(tab:comparison)

```{r comparison}

comparison.ft %>% set_caption("Various Performance Metrics Comparing Predictions on the Test Data from Adaptive LASSO Logistic Regression and Random Forest Models")

```




## Accuracy

Figure 
- ROC & Cost on test data

## Other Considerations

Transparency
...


## Selection





\newpage
# (PART) Discussion

# Scenario Application

***Do customers who already have an account with Bank XYZ receive any favorable treatment?***

Was it picked up in any of our models?
What advantage might it have that wouldn't be evident in data?

***Suppose a credit card application is rejected, how would you explain the results to the customer?***

For TP and FP, does it differ? 

Figure
- "Tipping point" plots (if we took any value from specific applicant and replaced with mean of someone who would have been approved, does it change probability enough to cross threshold? iterative combinations of strongest more variable predictors or z-score of particular customer?)




Figure \@ref(fig:example)

```{r example, fig.width=7.5, fig.height=9/16*7.5, fig.cap = "Explanation of A Rejected Applicant"}

include_graphics("../../04-Data-Visualization/02-Figures/rejection_example.png")


```







# Conclusions

Dilemma of basing credit approvals on default data (inherently missing approvals that should have been made, identifying only those that shouldn't have been)


\newpage
# References




```{r eval=F}
knitr::write_bib(x = 
                   c('knitr', 'rmarkdown', 'bookdown', 'dplyr', 'magrittr', 'randomForest', 
                     'parallel', 'doParallel', 'dlookr', 'tidyverse', 'Hmisc', 'corrplot', 
                     'brms', 'caret', 'here', 'skimr', 'DataExplorer', 'pROC', 'patchwork', 
                     'purrr', 'flextable', 'glmnet', 'tibble', 'doRNG'), 
                 file = "./00-References/packages.bib")
```







\newpage
# (APPENDIX) Appendix {-}

## Extended Exploratory Data Analysis




## Training Details

### Class Imbalance

### Cutoff Thresholds




## Cost Rationale




















# Table 1s


```{r}
flextable(table.1s[[3]]) %>% set_caption("Descriptive Statistics of Validation Data")
flextable(table.1s[[1]]) %>% set_caption("Descriptive Statistics of Test Data")




```